# 定义

> **MCP（Model Context Protocol）**：一种让“模型”和“外部工具 / 数据源 / 服务”进行交互的统一协议。

简单理解：

- 它定义了一套通用的“说话方式”和“接口格式”：
  - 模型如何发现有哪些工具可用；
  - 如何调用这些工具（发什么参数、收什么返回）；
  - 工具如何把结果再喂回给模型。
- 目标是：
  - **标准化** 工具接入方式，不再每家都自创一套插件协议；
  - 让模型更容易安全、可靠地访问：
    - 数据库
    - Web API
    - 本地文件
    - 第三方 SaaS 等等。

> MCP 是一套“模型 ↔ 工具 / 数据源”的通用协议标准，方便把各种外部能力挂到大模型上用。





# 所在位置

它主要是用在**“模型要访问外部数据 / 工具 / 服务”这一层**，也就是：

> **在「大模型客户端」和「外部工具 / 数据源」之间，当作通信协议和适配层来用。**

你可以把整条链路想成这样：

```arduino
用户 ⇄ ChatGPT / 你的应用（LLM Client）
                    ⇅
               MCP 协议层
                    ⇅
         各种 MCP server（数据库、SaaS、HTTP API、本地服务…）
```

更具体一点分几个位置讲：

------

### 1. 在「模型客户端」这一侧

例如：

- ChatGPT 网页 / Apps SDK / 你的自研 LLM 应用
- 这些东西都是 **MCP client**：

它们做的事情：

1. 通过 MCP 和一个或多个 MCP server 建立连接（stdio、HTTP、SSE 等）。
2. 拉取这些 server 暴露出来的 **tools 列表 + schema + 元数据**。
3. 当模型在对话中决定要用某个工具时，就按 MCP 协议发 `call_tool` 请求，带上参数。
4. 收到 MCP server 返回的结构化结果，再把结果喂回模型推理。

所以在客户端这边，MCP 就是**“统一调用各种工具/数据源的协议层”**。

------

### 2. 在你的后端 / 第三方服务这一侧（MCP server）

你的服务（或第三方服务）会实现一个 **MCP server**：

- 以 HTTP / SSE / stdio 等形式暴露；
- 在协议里声明：我有哪些 tools、各自参数 / 返回值的 JSON Schema、权限要求等；
- 当收到 `call_tool` 请求时，执行：
  - 查数据库
  - 调内部 microservice
  - 调三方 API（Stripe、Shopify、GitHub 等）
- 把结果按 MCP 规定的结构返回。

所以在后端这边，MCP server 就是**“给 LLM 暴露能力的适配器 / 网关”**。

------

### 4. 用一句话帮你记住

> MCP 不是“放在代码某一行”，而是**放在 LLM 和外部世界之间，作为统一的工具/数据访问协议层**：
>
> - 上接模型客户端（ChatGPT、Apps SDK、你的 LLM app）
> - 下接 MCP servers（数据库、业务系统、第三方 SaaS、任何 HTTP / 本地服务）

