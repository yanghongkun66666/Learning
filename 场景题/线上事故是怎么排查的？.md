## 一、线上事故怎么排查

不是要你背命令，而是要你体现：

1. **先稳态：止血优先**（影响面/风险控制）
2. **数据驱动：靠指标定位，不靠猜**
3. **分层思路：入口→链路→依赖→资源→数据**
4. **有方法：能复现、能回滚、能验证**
5. **有复盘：根因、改进、避免再次发生**

你回答时，最好用固定框架：**“先定级→止血→定位→修复→验证→复盘”**。

------

## 二、线上事故常见类型

按常见的分类，基本是这几类：

### 1) 可用性事故（最常见）

- 接口超时、错误率飙升（5xx/4xx异常增加）
- 服务不可用、实例频繁重启
- 网关/负载均衡异常

**典型原因**：流量突增、线程池/连接池耗尽、GC 抖动、依赖超时、死锁、DNS/网络。

### 2) 性能事故

- RT（响应时间）陡增
- CPU 飙高、Load 高
- 内存飙高/频繁 Full GC
- IO 飙高、磁盘满

**典型原因**：慢 SQL、缓存击穿、对象创建过多、日志爆量、热 key、JVM 配置不当。

### 3) 数据事故（最危险）

- 数据错乱、金额错误、幂等失败、重复扣款
- 数据延迟、数据丢失、写入失败
- 读写不一致、主从延迟

**典型原因**：事务/幂等设计缺失、消息重复/丢失、回放不当、DDL/脚本误操作。

### 4) 发布/变更事故（占比很高）

- 刚发布就炸：错误率上升、功能异常
- 配置错误、开关误开
- 依赖升级不兼容、灰度策略错误

**典型原因**：变更引入 bug、配置中心误配、回滚策略缺失。

### 5) 安全/合规类

- 账号异常、接口被刷、风控失效
- 证书过期、鉴权异常

### 6) 外部依赖事故

- DB、Redis、MQ、ES、第三方支付/短信/地图等抖动或挂掉
- 网络抖动、DNS 故障、跨机房链路异常

------

## 三、通用排查 SOP（小白直接照着做）

### Step 0：先做“事故定级 + 影响面”

你必须先回答清楚：

- 影响谁：全量还是部分用户？某城市/某机房？某版本？
- 影响多大：QPS、错误率、金额风险？
- 什么时候开始：是否与发布/变更时间重合？

**先确认事故范围和严重程度，决定是否立刻止血与升级响应。**

------

### Step 1：先止血（第一优先级）

可用的止血手段（按从快到慢）：

1. **回滚/降级**（回滚版本、回退配置）
2. **开关熔断**（关闭新功能、关闭非核心链路）
3. **限流**（保护核心链路）
4. **扩容**（临时加机器/加副本）
5. **隔离**（摘掉异常实例、隔离机房）
6. **降级返回**（兜底缓存/兜底结果/排队）

**先恢复服务，再找根因。不要在火场里做手术。**

------

### Step 2：用“三板斧”定位（看图、看链路、看变更）

#### 板斧 1：看监控（Metrics）

你要立刻打开：

- 错误率（5xx/异常数）
- RT（P50/P90/P99）
- QPS/流量
- 资源：CPU、内存、GC、线程数、连接池、磁盘、网络

经验规则：

- **错误率升 + RT升**：多为依赖超时/资源耗尽/线程池打满
- **错误率升 + RT不升**：多为代码逻辑异常/参数校验/配置错误
- **RT升 + 错误率不升**：多为性能问题（慢 SQL、锁、GC、IO）

#### 板斧 2：看链路（Tracing）

用链路追踪（SkyWalking/Jaeger/Zipkin/ARMS 等）看：

- 哪一段耗时最长（DB？Redis？RPC？）
- 哪个下游报错最多（依赖服务/第三方）

#### 板斧 3：看变更（Change）

立刻对齐时间线：

- 最近 30 分钟/2小时是否有：发布、配置、扩容、依赖升级、脚本执行？
- 大厂事故很多时候：**变更=80%线索**

------

### Step 3：按层排查（从入口到依赖）

1. **入口层**：DNS、LB、网关、WAF、Nginx
2. **应用层**：线程池/连接池、GC、日志、异常
3. **依赖层**：DB/Redis/MQ/ES/第三方
4. **数据层**：表锁、慢 SQL、主从延迟、消息堆积
5. **基础设施**：网络、磁盘、容器、节点故障

------

### Step 4：验证假设

每定位一个点，你要能回答：

- 我怎么证明它是根因？
- 修复后怎么确认恢复？
- 有没有副作用？

例如：

- 怀疑慢 SQL → 看慢查询日志/执行计划/锁等待
- 怀疑线程池打满 → 看线程池 active/queue/reject
- 怀疑 Redis 挂 → 看连接数、超时、命中率、延迟、热 key
- 怀疑 MQ 堆积 → 看 lag、消费速率、重试队列

------

### Step 5：修复与复盘

修复完要做：

- **回归验证**：错误率/RT/QPS 恢复正常
- **防复发**：加监控、限流、熔断、降级、压测、灰度、演练
- **复盘**：5 Whys（为什么）+ 行动项（Owner+截止时间）

------

## 四、典型事故场景 & 快速定位套路

### 场景1：RT 飙升 + 错误率飙升（大量超时）

优先怀疑：

- 依赖超时（DB/Redis/第三方）
- 线程池/连接池耗尽
- 下游雪崩导致级联

排查顺序：

1. 链路追踪看哪个 span 最慢
2. 看线程池/连接池指标（active/queue/timeout）
3. 看依赖延迟与错误
   止血：熔断下游、限流、降级、扩容

------

### 场景2：错误率上升但 RT 正常（快速失败）

优先怀疑：

- 发布 bug、配置错误、鉴权失败
- 依赖返回码变化（协议不兼容）

排查顺序：

1. 对齐变更（刚发布？配置改了？）
2. 看日志异常堆栈（按 traceId）
   止血：回滚/回退配置/关闭开关

------

### 场景3：CPU 飙高

常见原因：

- 死循环/热路径退化
- 序列化/正则灾难
- GC 频繁

排查：

1. 先看 GC 次数与耗时
2. 线程 dump / profile 看热点
   止血：扩容、限流、回滚

------

### 场景4：内存飙高/频繁 Full GC

原因：

- 内存泄漏、缓存无限增长
- 大对象/大集合、日志拼接

排查：

1. JVM GC 日志、堆使用曲线
2. heap dump 分析（MAT）
   止血：重启有风险，优先回滚/限流/临时扩容

------

### 场景5：数据库变慢/连接打满

原因：

- 慢 SQL、锁等待、连接泄漏、索引缺失

排查：

1. 慢查询、`show processlist`、锁等待
2. 应用连接池（active/max/等待）
   止血：限流、降级、临时加读库/扩容、kill 事务（谨慎）

------

### 场景6：缓存雪崩/击穿/穿透

现象：Redis 命中率掉、DB 压力暴涨、RT 飙升
排查：命中率、热点 key、过期策略
止血：热点 key 本地缓存、互斥锁/单飞、降级兜底、限流

------

### 场景7：MQ 堆积

现象：延迟上升、消费 lag 增大、数据处理不及时
排查：生产速率 vs 消费速率、失败重试队列
止血：扩容消费者、降级非关键消息、修复消费失败原因

------

## 五、怎么“组织”应对事故的

### 1) Incident 指挥体系

- **IC（Incident Commander）**：总指挥，负责协调、决策、对外同步
- **Ops/SRE**：基础设施、扩容、流量切换
- **Dev Owner**：应用定位修复
- **DBA/中间件 Owner**：依赖定位修复
- **Comms**：对业务方/客服同步进展（每 10/15 分钟一次）

### 2) 标准化工具链

- 统一监控（Prometheus/Grafana）、告警（PagerDuty/钉钉/飞书）
- 日志平台（ELK/SLS）、Trace（SkyWalking/Jaeger）
- 变更平台（发布记录、配置变更记录）
- Runbook（标准操作手册）

### 3) 事故后必须复盘

- 时间线、影响面、根因、为什么没提前发现
- 行动项：监控补齐、限流熔断降级、压测、演练、灰度、回滚机制

------

## 六、回答模板

> “线上出事故我会按 SOP：先确认影响面和定级，然后优先止血，比如回滚/降级/限流/熔断/扩容，确保核心链路恢复。接着用监控看错误率、RT、QPS 和资源指标，结合链路追踪定位是应用层还是依赖层的问题，并对齐最近发布/配置/依赖变更作为关键线索。定位后验证假设，通过日志+traceId+依赖指标确认根因，再做修复与灰度验证，最后复盘补监控、完善降级与演练，避免复发。”

